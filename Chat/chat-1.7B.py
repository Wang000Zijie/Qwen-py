#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# qwen_chat.py - Qwen3 æœ¬åœ°å¯¹è¯è„šæœ¬ï¼ˆLinux ä¸“ç”¨ï¼‰ - å·²å¯ç”¨ FlashAttention-2 + ç¼–è¯‘ç¼“å­˜

import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer
from transformers import TextIteratorStreamer
import os
import threading
import time

# ==================== ğŸ”§ å¯é…ç½®å‚æ•° ====================
# ä¿®æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
model_path = "/home/balcony/models/Qwen3-1.7B"

# å¯¹è¯ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
save_dir = "/home/balcony/Qwen/cb"
os.makedirs(save_dir, exist_ok=True)

# ç”Ÿæˆå‚æ•°ï¼ˆQwen å®˜æ–¹æ¨èï¼‰
temperature      = 0.7
top_p            = 0.8
top_k            = 20
min_p            = 0.0
presence_penalty = 1.1
max_new_tokens   = 1024           # âœ… å¯è®¾ä»»æ„å€¼ï¼š256, 512, 768, 1024...
max_history_rounds = 2           # âœ… å‡å°‘å†å²è½®æ•°ï¼Œæå‡é€Ÿåº¦

# ç³»ç»Ÿæç¤ºè¯­
system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šå’Œå‹å¥½ã€‚"
# ======================================================

# æ£€æŸ¥æ¨¡å‹è·¯å¾„
if not os.path.exists(model_path):
    raise FileNotFoundError(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

print("ğŸ” æ£€æŸ¥æ¨¡å‹è·¯å¾„:", model_path)
print("ğŸ“‚ æ¨¡å‹ç›®å½•å†…å®¹:", os.listdir(model_path)[:3])

# ==================== åŠ è½½æ¨¡å‹ ====================
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    local_files_only=True  # å¼ºåˆ¶æœ¬åœ°åŠ è½½
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="cuda:0",                    # å¼ºåˆ¶ä½¿ç”¨ GPU
    attn_implementation="flash_attention_2", # ğŸ”¥ å¯ç”¨ FlashAttention-2
    offload_folder=None,                    # ç¦ç”¨ offload
    local_files_only=True
).eval()

# âœ… ç¼–è¯‘æ¨¡å‹ + å¯ç”¨ç£ç›˜ç¼“å­˜ï¼ˆé¦–æ¬¡æ…¢ï¼Œåç»­é£å¿«ï¼‰
compile_cache_dir = "/home/balcony/.cache/torch_compile"
os.makedirs(compile_cache_dir, exist_ok=True)

try:
    print("ğŸ”¥ ç¼–è¯‘æ¨¡å‹ä¸­ï¼ˆé¦–æ¬¡è¿è¡Œç¨æ…¢ï¼Œåç»­å°†ä»ç¼“å­˜åŠ è½½ï¼‰...")
    model = torch.compile(
        model,
        mode="reduce-overhead",
        fullgraph=True,
        dynamic=False,
        cache_dir=compile_cache_dir  # âœ… å…³é”®ï¼šç¼–è¯‘ç»“æœå­˜ç¡¬ç›˜
    )
    print("âœ… æ¨¡å‹ç¼–è¯‘å®Œæˆï¼Œåç»­è¿è¡Œå°†æ˜¾è‘—åŠ é€Ÿ")
except Exception as e:
    print(f"âš ï¸ ç¼–è¯‘å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰\n")

# ==================== å¯¹è¯å†å² ====================
history = []

def stream_generate(input_ids):
    """æµå¼ç”Ÿæˆå‡½æ•°"""
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True,
        timeout=10.0
    )
    gen_kwargs = {
        "input_ids": input_ids,
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "min_p": min_p,
        "repetition_penalty": presence_penalty,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.eos_token_id,
        "streamer": streamer,
    }
    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)
    thread.start()
    return streamer

# ==================== ä¸»å¾ªç¯ ====================
while True:
    try:
        user_input = input("User: ").strip()
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ å†è§ï¼")
            break

        # âœ… é™åˆ¶å†å²è½®æ•°ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡çˆ†ç‚¸
        recent_history = history[-max_history_rounds:]

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": system_prompt}]
        for u, a in recent_history:
            messages.append({"role": "user", "content": u})
            messages.append({"role": "assistant", "content": a})
        messages.append({"role": "user", "content": user_input})

        # åº”ç”¨æ¨¡æ¿
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # æµå¼è¾“å‡º
        print("Assistant: ", end="", flush=True)
        start_time = time.time()
        streamer = stream_generate(inputs.input_ids)

        response = ""
        token_count = 0
        for new_text in streamer:
            print(new_text, end="", flush=True)
            response += new_text
            token_count += 1  # ç²—ç•¥è®¡æ•°
        print()

        # âœ… æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦
        if token_count > 0:
            duration = time.time() - start_time
            speed = token_count / duration
            print(f"â±ï¸ ç”Ÿæˆ {token_count} ä¸ª tokenï¼Œè€—æ—¶ {duration:.2f}s â†’ {speed:.2f} token/s")

        # ä¿å­˜å¯¹è¯
        history.append((user_input, response))
        if len(history) > 10:
            history = history[-10:]

    except KeyboardInterrupt:
        print("\nâŒ ç”Ÿæˆè¢«ä¸­æ–­")
    except torch.cuda.OutOfMemoryError:
        print("\nâŒ æ˜¾å­˜ä¸è¶³ï¼è‡ªåŠ¨æ¸…ç†ç¼“å­˜...")
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")